# Code Implementation Progress Summary
*Accumulated implementation progress for all files*


================================================================================
## IMPLEMENTATION File requirements.txt; ROUND 0 
================================================================================

# Code Implementation Summary
**Generated**: 2025-12-08 10:16:58
**File Implemented**: requirements.txt

**Core Purpose**
- Defines the specific Python package versions and libraries required to set up the execution environment for the project, ensuring compatibility for model inference (Torch, Transformers), API interaction (OpenAI), data handling (Datasets, Pandas), and evaluation metrics (NLTK, Rouge-Score).

**Public Interface**
- **N/A**: This file is consumed by the package manager (`pip`) rather than other code files.
- **Key Libraries**: `torch`, `transformers`, `accelerate`, `openai`, `datasets`, `nltk`, `rouge_score`, `scikit-learn`, `numpy`, `pyyaml`.

**Internal Dependencies**
- **N/A**: Does not import other files.

**External Dependencies**
- **Expected to be used by**: The entire project infrastructure relies on these packages being installed.
- **Key exports used elsewhere**:
  - `transformers` & `torch`: Used by `src/core/llm_wrapper.py` and `src/core/defenses.py`.
  - `openai`: Used by `src/core/llm_wrapper.py`.
  - `datasets`: Used by `src/data/loader.py`.
  - `pyyaml`: Used to parse `config/tasks.yaml` and `config/models.yaml`.

**Implementation Notes**
- **Version Constraints**: Specifies minimum versions (e.g., `torch>=2.0.0`) to ensure support for recent features like `accelerate` integration and specific API changes in `openai`.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File config/tasks.yaml; ROUND 0 
================================================================================

# Code Implementation Summary
**Generated**: 2025-12-08 10:17:17
**File Implemented**: config/tasks.yaml

**Core Purpose**
Defines the configuration for the 7 NLP tasks used in the benchmark (MRPC, Jfleg, HSOL, RTE, SST2, SMS, Gigaword). It serves as the central registry for dataset sources, prompt templates, input field mappings, and label definitions required to standardize inputs for the LLM.

**Public Interface**
- **YAML Structure**: A dictionary where keys are task identifiers (`mrpc`, `jfleg`, `hsol`, `rte`, `sst2`, `sms`, `gigaword`).
- **Task Schema**:
  - `dataset_name`: HuggingFace dataset identifier (e.g., "glue", "sms_spam").
  - `subset`: Specific configuration name (e.g., "sst2") or `null`.
  - `prompt_template`: String with placeholders (e.g., `{text}`) defining the instruction.
  - `input_fields`: List of dataset column names to map to prompt placeholders.
  - `label_map`: Dictionary mapping integer labels to string outputs (e.g., `{0: "negative", 1: "positive"}`) for classification tasks.
  - `task_type`: String identifier ("classification" or "generation").

**Internal Dependencies**
- None (Configuration file).

**External Dependencies**
- Expected to be imported by: `src/data/loader.py` (to load and format datasets), `src/data/sampler.py` (to access label maps for sampling), and `scripts/run_benchmark.py` (to iterate through tasks).

**Implementation Notes**
- **Label Mapping**: Explicitly maps integer class labels to natural language strings (e.g., 0 -> "yes") to facilitate direct comparison with LLM text generation outputs.
- **Prompt Fidelity**: Implements the exact prompt templates specified in Table 11 of the reference paper.
- **Dataset Handling**: Distinguishes between tasks requiring subsets (GLUE tasks) and those that do not (SMS, Gigaword).

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File config/models.yaml; ROUND 0 
================================================================================

# Code Implementation Summary
**Generated**: 2025-12-08 10:17:35
**File Implemented**: config/models.yaml

**Core Purpose**
This configuration file defines the operational parameters for the Large Language Models (LLMs) used in the framework. It separates settings for API-based models (OpenAI) and locally hosted models (HuggingFace), ensuring consistent hyperparameters like temperature and token limits across experiments.

**Public Interface**
*   **YAML Schema**:
    *   `models.openai`: Configuration for OpenAI API usage.
        *   `api_key_env`: Environment variable name for the API key (e.g., "OPENAI_API_KEY").
        *   `default_model`: The model identifier string (e.g., "gpt-3.5-turbo-instruct").
        *   `temperature`: Sampling temperature (set to 0.1).
    *   `models.local`: Configuration for local HuggingFace models.
        *   `default_model`: HuggingFace model path (e.g., "meta-llama/Llama-2-7b-chat-hf").
        *   `device`: Compute device ("cuda" or "cpu").
        *   `temperature`: Sampling temperature (set to 0.1).
        *   `max_new_tokens`: Generation limit (set to 100).

**Internal Dependencies**
*   None (Static configuration file).

**External Dependencies**
*   **Expected to be imported by**:
    *   `src/core/llm_wrapper.py`: Will read this file to initialize model instances with the specified parameters.
    *   `scripts/run_benchmark.py`: May reference model names for logging or setup.

**Implementation Notes**
*   **Architecture Decisions**: Separates `openai` and `local` configs to allow the `LLMWrapper` class to switch backends easily based on user input or availability.
*   **Cross-File Relationships**: This file works in tandem with `src/core/llm_wrapper.py`, which will parse these values to instantiate the actual model objects.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/utils/text_processing.py; ROUND 1 
================================================================================

# Code Implementation Summary
**Generated**: 2025-12-08 10:18:32
**File Implemented**: src/utils/text_processing.py



---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/utils/logger.py; ROUND 2 
================================================================================

# Code Implementation Summary
**Generated**: 2025-12-08 10:19:23
**File Implemented**: src/utils/logger.py

**Core Purpose**
Provides a standardized logging utility to capture experiment execution details, errors, and results, writing output simultaneously to the console and timestamped log files to ensure reproducibility and debugging capability.

**Public Interface**
- Function `setup_logger(name="benchmark", log_dir="logs", level=logging.INFO)` -> `logging.Logger`: Configures and returns a logger instance with formatted file and stream handlers.

**Internal Dependencies**
- Standard Library: `logging`, `os`, `sys`, `datetime`

**External Dependencies**
- Expected to be imported by: `scripts/run_benchmark.py`, `scripts/calibrate_defense.py`, `src/core/llm_wrapper.py`, and other core modules requiring execution tracking.

**Implementation Notes**
- Architecture decisions: Includes logic to clear existing handlers to prevent duplicate log entries if the logger is initialized multiple times within the same process.
- Cross-File Relationships: Acts as a foundational utility used across the entire project to unify output formatting.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/data/loader.py; ROUND 3 
================================================================================

# Code Implementation Summary
**Generated**: 2025-12-08 10:20:26
**File Implemented**: src/data/loader.py

**Core Purpose**
Manages the loading, formatting, and preprocessing of the 7 benchmark datasets (MRPC, Jfleg, HSOL, RTE, SST2, SMS, Gigaword). It integrates with the HuggingFace `datasets` library to retrieve data and applies task-specific prompt templates and label mappings defined in the configuration to prepare data for the prompt injection experiments.

**Public Interface**
- Class `DataLoader`: Central handler for dataset operations.
  - Constructor: `__init__(config_path="config/tasks.yaml")`
  - Method: `load_task_data(task_name, split="validation", limit=None)` -> `list[dict]`
    - Purpose: Loads and formats a specific dataset.
    - Returns: List of dictionaries with keys: `'original_inputs'` (dict), `'formatted_input'` (str), and `'label'` (str).

**Internal Dependencies**
- From `src.utils.logger`: `setup_logger`
- From `src.utils.text_processing`: `clean_text`
- Reads file: `config/tasks.yaml` (runtime dependency for task definitions)

**External Dependencies**
- Expected to be imported by: `src/data/sampler.py` (for creating attack pairs), `scripts/run_benchmark.py` (for loading evaluation data), `scripts/calibrate_defense.py`.
- External packages: `datasets` (HuggingFace), `yaml`.

**Implementation Notes**
- **Architecture decisions**: Abstracts away dataset-specific irregularities (e.g., `hsol` and `sms` lacking standard validation splits) to provide a consistent API for the benchmark runner.
- **Cross-File Relationships**: Tightly coupled with `config/tasks.yaml`; the loader dynamically reads input fields and prompt templates from the config, meaning changes to the YAML file immediately affect data loading behavior without code changes.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/data/sampler.py; ROUND 4 
================================================================================

# Code Implementation Summary
**Generated**: 2025-12-08 10:21:34
**File Implemented**: src/data/sampler.py

**Core Purpose**
This file implements the logic for creating evaluation datasets by sampling pairs of "target" (benign) and "injected" (malicious) inputs. Its primary responsibility is to ensure that for classification tasks, the ground truth labels of the target and injected inputs are distinct, enabling clear attribution of the model's output to either the target or the injected task.

**Public Interface**
- Class `TaskSampler`: Manages data loading and pair sampling.
  - Constructor: `__init__(config_path: str = "config/tasks.yaml")`
  - Method: `sample_pairs(target_task: str, injected_task: str, num_samples: int = 100, seed: int = 42) -> List[Dict[str, Any]]`: Returns a list of dictionaries, each containing a 'target' sample and an 'injected' sample.

**Internal Dependencies**
- From `src.data.loader`: `DataLoader` (used to fetch raw dataset entries).
- From `src.utils.logger`: `setup_logger` (for logging sampling progress and warnings).
- External packages: `random` (for shuffling and sampling), `logging`.

**External Dependencies**
- Expected to be imported by: `scripts/run_benchmark.py` (to generate the 7x7 evaluation matrix), `scripts/demo_attack.py`.
- Key exports used elsewhere: The `sample_pairs` method is the primary entry point for generating test cases.

**Implementation Notes**
- **Label Constraint**: The sampler explicitly checks `if t_label == i_label` and skips such pairs. This is critical for the "Matching Rate" and "Attack Success Value" metrics, as it removes ambiguity when the model outputs a class label (e.g., "Positive").
- **Caching**: Implements `self.cache` to store loaded datasets, preventing redundant disk I/O or API calls when sampling multiple combinations involving the same task.
- **Resampling Strategy**: Uses a pointer-based approach with reshuffling to ensure unique pairings and maximize the usage of available data without infinite loops.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/core/llm_wrapper.py; ROUND 5 
================================================================================

# Code Implementation Summary
**Generated**: 2025-12-08 10:22:33
**File Implemented**: src/core/llm_wrapper.py

**Core Purpose**
Provides a unified abstraction layer for interacting with Large Language Models, supporting both OpenAI API (GPT-3.5/4) and local HuggingFace models (e.g., Llama-2). It handles model initialization, text generation queries, and perplexity calculations required for detection-based defenses.

**Public Interface**
- **Class `LLMWrapper`**: Manages model interaction.
  - `__init__(model_type: str = "local", config_path: str = "config/models.yaml")`: Initializes the model based on type ("openai" or "local") and configuration.
  - `query(prompt: str, max_tokens: Optional[int] = None) -> str`: Generates text response for a given prompt, abstracting API vs local inference details.
  - `get_perplexity(text: str) -> float`: Calculates the perplexity of the input text (Local models only), used for PPL detection.
  - `get_windowed_perplexity(text: str, window_size: int = 50) -> float`: Calculates the maximum perplexity over sliding windows (Local models only).

**Internal Dependencies**
- **From `src.utils.logger`**: `setup_logger` for structured logging.
- **External packages**: 
  - `openai`: For API-based model interaction.
  - `transformers`, `torch`: For loading and running local models and calculating perplexity.
  - `yaml`: For loading model configurations.

**External Dependencies**
- **Expected to be imported by**: 
  - `src/core/defenses.py`: For Paraphrasing (generation) and PPL Detection (perplexity).
  - `scripts/run_benchmark.py`: To generate responses from models during the attack simulation.
  - `scripts/calibrate_defense.py`: To calculate perplexity thresholds on clean data.

**Implementation Notes**
- **Architecture**: Uses a strategy pattern internally (`_query_openai` vs `_query_local`) to handle different backends while exposing a single API.
- **PPL Calculation**: Implements specific logic for calculating perplexity and windowed perplexity using PyTorch, which is critical for the "PPL Detection" defense strategy defined in the paper.
- **Error Handling**: Includes conditional imports and try-catch blocks to handle API failures or missing dependencies gracefully.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/core/attacks.py; ROUND 6 
================================================================================

# Code Implementation Summary
**Generated**: 2025-12-08 10:23:15
**File Implemented**: src/core/attacks.py

**Core Purpose**
Defines the abstract interface and concrete implementations for five specific prompt injection attack strategies (Naive, Escape Characters, Context Ignoring, Fake Completion, and Combined). It serves as the mechanism to generate adversarial inputs (`x_tilde`) by combining target data, injected instructions, and injected data according to the paper's formal definitions.

**Public Interface**
- **Class `AttackStrategy` (ABC)**: Abstract base class for all attacks.
  - Method `apply(target_input: str, injected_instruction: str, injected_input: str) -> str`: Generates the adversarial string.
  - Property `name`: Returns the unique identifier of the strategy.
- **Classes `NaiveAttack`, `EscapeCharactersAttack`, `ContextIgnoringAttack`, `FakeCompletionAttack`, `CombinedAttack`**: Concrete implementations of `AttackStrategy`.
- **Function `get_attack_strategy(name: str) -> AttackStrategy`**: Factory function to instantiate an attack strategy by its registry name.
- **Constant `ATTACK_REGISTRY`**: Dictionary mapping string names (e.g., "combined") to attack classes.

**Internal Dependencies**
- **Standard Library**: `abc` (Abstract Base Classes), `typing`.

**External Dependencies**
- **Expected to be imported by**: 
  - `scripts/run_benchmark.py` (to generate attacks during evaluation)
  - `scripts/demo_attack.py` (for single-instance demonstrations)

**Implementation Notes**
- **Architecture**: Uses the Strategy Pattern to allow interchangeable attack logic.
- **Formulas**: The string concatenation logic in the `apply` methods strictly adheres to the mathematical formulations provided in the paper (e.g., `CombinedAttack` explicitly combines newline characters, fake completion headers, and context ignoring instructions).

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/core/defenses.py; ROUND 7 
================================================================================

# Code Implementation Summary
**Generated**: 2025-12-08 10:25:02
**File Implemented**: src/core/defenses.py

**Core Purpose**
This file implements the defense mechanisms against prompt injection attacks, categorized into **Prevention** (input transformation) and **Detection** (binary classification). It provides a unified abstract base class and a factory function to instantiate specific strategies like Paraphrasing, Retokenization, and Perplexity-based detection.

**Public Interface**
- **Class `DefenseStrategy` (ABC)**: Abstract base for all defenses.
  - `apply(target_instruction, input_text, llm=None) -> Dict[str, Any]`: Applies the defense. Returns a dictionary with `is_detected` (bool) and `prompt` (str).
- **Prevention Classes**:
  - `ParaphrasingDefense`: Uses an LLM to paraphrase the input.
  - `RetokenizationDefense`: Uses a tokenizer (default GPT-2) to break adversarial token sequences.
  - `DelimitersDefense`: Wraps input in delimiters (e.g., `'''`).
  - `SandwichDefense`: Appends the task instruction again after the input.
  - `InstructionalDefense`: Prepends a warning about malicious inputs.
- **Detection Classes**:
  - `PPLDetectionDefense`: Flags input if Perplexity > threshold.
  - `WindowedPPLDetectionDefense`: Flags input if max windowed Perplexity > threshold.
  - `KnownAnswerDefense`: Uses a "canary" prompt to check if the model follows instructions.
- **Function `get_defense_strategy(name: str, **kwargs) -> DefenseStrategy`**: Factory function to retrieve a defense instance by name.
- **Registry**: `DEFENSE_REGISTRY` maps string names (e.g., "paraphrasing", "ppl_detection") to classes.

**Internal Dependencies**
- From `src.core.llm_wrapper`: `LLMWrapper` (used for calculating PPL and generating paraphrases/canary responses).
- From `src.utils.logger`: `setup_logger`.

**External Dependencies**
- **External Packages**: `transformers` (AutoTokenizer), `torch`, `abc`, `random`, `string`.
- **Expected Consumers**: `scripts/run_benchmark.py` (to apply defenses during evaluation), `scripts/calibrate_defense.py` (to tune PPL thresholds).

**Implementation Notes**
- **Architecture**: Uses a Strategy pattern via the `DefenseStrategy` ABC, allowing the benchmark script to swap defenses easily.
- **PPL Defenses**: The `PPLDetectionDefense` and `WindowedPPLDetectionDefense` rely on the `LLMWrapper` exposing `get_perplexity` and `get_windowed_perplexity` methods.
- **Retokenization**: Defaults to `gpt2` tokenizer if no model name is provided; handles tokenizer loading failures gracefully by falling back to identity.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/evaluation/calibrator.py; ROUND 8 
================================================================================

# Code Implementation Summary
**Generated**: 2025-12-08 10:26:05
**File Implemented**: src/evaluation/calibrator.py

**Core Purpose**
This file implements the `ThresholdCalibrator` class, which is responsible for calculating Perplexity (PPL) thresholds for detection-based defenses. It analyzes clean data samples to determine cutoff values that satisfy a specific False Positive Rate (FPR) target (e.g., 1%), ensuring defenses do not over-flag legitimate inputs.

**Public Interface**
- **Class `ThresholdCalibrator`**: Manages the calibration process using an LLM and data loader.
  - **Constructor**: `__init__(self, llm_wrapper, data_loader)`
  - **Method**: `calibrate(self, tasks: List[str], limit: int = 100, fpr_target: float = 0.01) -> Dict[str, Dict[str, float]]`
    - Calculates standard and windowed PPL thresholds for the given tasks.
  - **Method**: `save_thresholds(self, thresholds: Dict, filepath: str = "config/defense_thresholds.json")`
    - Persists the calibrated thresholds to a JSON file.
  - **Method**: `load_thresholds(self, filepath: str = "config/defense_thresholds.json") -> Dict`
    - Loads previously calibrated thresholds.

**Internal Dependencies**
- **From `src.utils.logger`**: `setup_logger` for logging progress and errors.
- **External packages**: 
  - `numpy`: Used to calculate percentiles for threshold determination.
  - `json`, `os`: For file I/O operations.
- **Runtime Dependencies**: Requires instances of `LLMWrapper` (for `get_perplexity`, `get_windowed_perplexity`) and `DataLoader` (for `load_task_data`) passed during initialization.

**External Dependencies**
- **Expected to be imported by**: 
  - `scripts/calibrate_defense.py`: The script that executes the calibration process.
  - `src/core/defenses.py`: Likely uses `load_thresholds` to apply the calibrated values during defense execution.

**Implementation Notes**
- **Architecture**: The calibrator uses the validation split of datasets as "clean" data to avoid contaminating the test set.
- **Logic**: It calculates the `(1 - fpr_target)` percentile of PPL scores. For a 1% FPR target, it selects the 99th percentile score; any input with PPL higher than this is flagged as malicious.
- **Fallback**: If calibration fails for a task (e.g., no data), it defaults to a very high threshold (10000.0) to effectively disable the defense rather than crashing.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/evaluation/metrics.py; ROUND 9 
================================================================================

# Code Implementation Summary
**Generated**: 2025-12-08 10:27:27
**File Implemented**: src/evaluation/metrics.py

**Core Purpose**
Provides a unified interface to calculate evaluation metrics (Accuracy, Rouge-1, GLEU) for assessing model performance. It is specifically designed to compute the key benchmark metrics: Attack Success Value (ASV), Matching Rate (MR), and Performance on Clean Data (PNA) across both classification and generation tasks.

**Public Interface**
- **Class `MetricsCalculator`**: Handles the initialization of scorers and computation of metrics.
  - `__init__(self)`: Initializes the Rouge scorer.
  - `calculate(self, predictions: List[str], references: List[str], task_type: str) -> float`: Returns the primary metric score (Accuracy for classification, Rouge-1 for generation) as a float (0.0-1.0).
  - `get_all_metrics(self, predictions: List[str], references: List[str], task_type: str) -> Dict[str, float]`: Returns a dictionary containing all relevant metrics for the task (e.g., `{'rouge1': ..., 'gleu': ...}`).

**Internal Dependencies**
- **External packages**: 
  - `nltk`: Used for tokenization (`punkt`) and GLEU score calculation (`sentence_gleu`).
  - `rouge_score`: Used for calculating Rouge-1 F-measure.
  - `numpy`: Used for mean calculation.
  - `sklearn.metrics`: Imported but custom logic is used for accuracy to handle string matching nuances.

**External Dependencies**
- **Expected to be imported by**: 
  - `scripts/run_benchmark.py`: To evaluate attack success and defense utility.
  - `scripts/calibrate_defense.py`: Potentially used to verify utility preservation during calibration.

**Implementation Notes**
- **Accuracy Logic**: Implements a "relaxed" accuracy check (`_calculate_accuracy`) that counts a prediction as correct if it exactly matches the reference or starts with the reference string (handling common instruction-tuned model outputs like "Answer: positive").
- **Generation Metrics**: Defaults to Rouge-1 as the primary metric for generation tasks but also computes GLEU for a more robust evaluation of fluency and overlap.
- **Error Handling**: Includes fallback mechanisms for NLTK data downloading and tokenization failures.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File scripts/calibrate_defense.py; ROUND 11 
================================================================================

# Code Implementation Summary
**Generated**: 2025-12-08 10:28:34
**File Implemented**: scripts/calibrate_defense.py

**Core Purpose**
This script serves as the standalone entry point for calibrating Perplexity (PPL) detection thresholds. It orchestrates the process of loading clean validation data, calculating perplexity scores using a reference LLM, and determining specific threshold values that maintain a target False Positive Rate (FPR) (e.g., 1%) to be used in defense mechanisms.

**Public Interface**
*   **CLI Script**: Executed via command line.
*   **Arguments**:
    *   `--tasks`: Comma-separated list of tasks or "all" (default: "all").
    *   `--model_type`: "local" or "openai" (default: "local").
    *   `--fpr`: Target False Positive Rate (default: 0.01).
    *   `--limit`: Number of samples per task (default: 100).
    *   `--output_path`: Path to save the resulting JSON (default: "config/defense_thresholds.json").
    *   `--model_config`, `--task_config`: Paths to configuration files.

**Internal Dependencies**
*   From `src.utils.logger`: `setup_logger` for structured logging.
*   From `src.core.llm_wrapper`: `LLMWrapper` for model access.
*   From `src.data.loader`: `DataLoader` for fetching clean validation samples.
*   From `src.evaluation.calibrator`: `ThresholdCalibrator` which contains the core calibration logic.
*   External packages: `argparse`, `yaml`, `sys`, `os`.

**External Dependencies**
*   **Output Consumer**: The JSON file generated (`config/defense_thresholds.json`) is expected to be loaded by `src/core/defenses.py` or `scripts/run_benchmark.py` to configure the PPL detection defense during evaluation.

**Implementation Notes**
*   **Workflow**: Initializes components -> Resolves task list -> Runs `calibrator.calibrate()` -> Saves results to JSON.
*   **Error Handling**: Includes try-except blocks to gracefully handle initialization or runtime errors during the calibration loop.
*   **Configuration**: Relies on `config/tasks.yaml` to expand the "all" tasks argument into a specific list of task names.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File scripts/run_benchmark.py; ROUND 12 
================================================================================

# Code Implementation Summary
**Generated**: 2025-12-08 10:30:16
**File Implemented**: scripts/run_benchmark.py

**Core Purpose**
Orchestrates the comprehensive prompt injection benchmark by iterating through a configurable matrix of target tasks, injected tasks, attack strategies, and defense mechanisms. It manages the end-to-end workflow of data sampling, adversarial prompt generation, model querying, and metric calculation (ASV and MR) to evaluate system robustness.

**Public Interface**
- **CLI Script**: Executed via `python scripts/run_benchmark.py [options]`
- **Key Arguments**:
  - `--target_tasks`, `--injected_tasks`: Comma-separated lists or "all" to define the task matrix.
  - `--attacks`, `--defenses`: Comma-separated lists to select specific strategies (supports "none" for defenses).
  - `--model_type`: Choice of "local" or "openai".
  - `--num_samples`: Number of non-overlapping pairs to evaluate per combination (default: 100).
  - `--defense_thresholds`: Path to JSON file containing calibrated PPL thresholds.

**Internal Dependencies**
- From `src.utils.logger`: `setup_logger` for experiment tracking.
- From `src.core.llm_wrapper`: `LLMWrapper` for unified model inference.
- From `src.core.attacks`: `get_attack_strategy`, `ATTACK_REGISTRY` for generating adversarial inputs.
- From `src.core.defenses`: `get_defense_strategy`, `DEFENSE_REGISTRY` for applying protection layers.
- From `src.data.sampler`: `TaskSampler` for generating (Target, Injected) dataset pairs.
- From `src.evaluation.metrics`: `MetricsCalculator` for computing ASV and MR scores.
- Standard Libraries: `argparse`, `json`, `os`, `sys`, `yaml`, `tqdm`.

**External Dependencies**
- None (Top-level execution script).

**Implementation Notes**
- **Experiment Loop**: Implements a nested loop structure (Target Task → Injected Task → Defense → Attack) to cover the full experimental matrix defined in the paper.
- **Baseline Generation**: Pre-calculates "clean injected outputs" (what the model would say if asked the injected task directly) for every sample batch to accurately compute the Matching Rate (MR) metric.
- **Prompt Construction**: Manually assembles prompts (`s_t + x_tilde`) when no defense is active, or delegates construction to the specific `Defense.apply` method when a defense is selected.
- **Resilience**: Saves intermediate results (`benchmark_results_partial.json`) after every attack/defense combination to prevent data loss during long-running benchmarks.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File scripts/demo_attack.py; ROUND 13 
================================================================================

# Code Implementation Summary
**Generated**: 2025-12-08 10:31:08
**File Implemented**: scripts/demo_attack.py

**Core Purpose**
Provides a standalone command-line utility to demonstrate a single instance of a prompt injection attack. It visualizes the entire pipeline: sampling a target/injected pair, constructing the attack payload, applying optional defenses, and querying the LLM to show the final output.

**Public Interface**
- **Script Entry Point**: `main()`
  - **CLI Arguments**:
    - `--target_task`: Name of the benign task (default: "sst2").
    - `--injected_task`: Name of the malicious task (default: "sms").
    - `--attack`: Attack strategy to apply (e.g., "combined", "naive").
    - `--defense`: Defense strategy to apply (default: "none").
    - `--model_type`: Backend to use ("local" or "openai").
    - `--seed`: Random seed for reproducibility.

**Internal Dependencies**
- **From `src.core.attacks`**: `get_attack_strategy`, `ATTACK_REGISTRY` (to generate adversarial inputs).
- **From `src.core.defenses`**: `get_defense_strategy`, `DEFENSE_REGISTRY` (to attempt detection/prevention).
- **From `src.core.llm_wrapper`**: `LLMWrapper` (to perform inference).
- **From `src.data.sampler`**: `TaskSampler` (to load specific dataset examples).
- **From `src.utils.logger`**: `setup_logger` (for status reporting).
- **External packages**: `argparse`, `yaml`, `sys`, `os`.

**External Dependencies**
- None (This is a top-level execution script).

**Implementation Notes**
- **Visualization**: Prints a 6-step breakdown of the process (Target Input -> Injected Instruction -> Injected Input -> Attacked Input -> Final Prompt -> Model Output) to the console for educational/debugging purposes.
- **Defense Simulation**: If a detection defense (like PPL) flags an attack, the script simulates a "[BLOCKED BY DEFENSE]" response rather than querying the LLM, mimicking a real-world guardrail.
- **Prompt Extraction**: Approximates the "instruction" ($s_e$) of the injected task by stripping the placeholder from the prompt template defined in `config/tasks.yaml`.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/__init__.py; ROUND 14 
================================================================================

# Code Implementation Summary
**Generated**: 2025-12-08 10:31:29
**File Implemented**: src/__init__.py

**Core Purpose**
This file serves as the package initializer for the `src` directory, marking it as a Python package to allow submodules (`core`, `data`, `evaluation`, `utils`) to be imported by external scripts.

**Public Interface**
- **None**: The file is currently empty (containing only a comment) and does not export specific symbols, relying on direct submodule imports instead.

**Internal Dependencies**
- **None**

**External Dependencies**
- **Required by**: All scripts in the `scripts/` directory (e.g., `run_benchmark.py`, `calibrate_defense.py`) rely on this file to resolve imports like `from src.core...` or `from src.data...`.

**Implementation Notes**
- **Architecture**: Kept empty to maintain a standard package structure without enforcing a specific top-level API surface at the root `src` level.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/core/__init__.py; ROUND 15 
================================================================================

# Code Implementation Summary
**Generated**: 2025-12-08 10:32:00
**File Implemented**: src/core/__init__.py

**Core Purpose**
This file serves as the package initializer for the `src.core` module, aggregating and exposing the primary classes and functions for LLM interaction, attack strategies, and defense mechanisms. It simplifies import statements for consumer scripts by providing a unified entry point to the core logic.

**Public Interface**
- **LLM Components**:
  - Class `LLMWrapper`: Unified interface for OpenAI and HuggingFace models.
- **Attack Components**:
  - Base Class `AttackStrategy`: Abstract base for attacks.
  - Classes `NaiveAttack`, `EscapeCharactersAttack`, `ContextIgnoringAttack`, `FakeCompletionAttack`, `CombinedAttack`: Specific attack implementations.
  - Function `get_attack_strategy(name, **kwargs)`: Factory function to instantiate attacks.
  - Constant `ATTACK_REGISTRY`: Dictionary mapping attack names to classes.
- **Defense Components**:
  - Base Class `DefenseStrategy`: Abstract base for defenses.
  - Classes `ParaphrasingDefense`, `RetokenizationDefense`, `DelimitersDefense`, `SandwichDefense`, `InstructionalDefense`: Prevention defenses.
  - Classes `PPLDetectionDefense`, `WindowedPPLDetectionDefense`, `KnownAnswerDefense`: Detection defenses.
  - Function `get_defense_strategy(name, **kwargs)`: Factory function to instantiate defenses.
  - Constant `DEFENSE_REGISTRY`: Dictionary mapping defense names to classes.

**Internal Dependencies**
- From `.llm_wrapper`: Imports `LLMWrapper`.
- From `.attacks`: Imports attack classes, factory function, and registry.
- From `.defenses`: Imports defense classes, factory function, and registry.

**External Dependencies**
- **Expected to be imported by**:
  - `scripts/run_benchmark.py`: To access attacks, defenses, and the LLM wrapper for the main experiment loop.
  - `scripts/demo_attack.py`: To load specific attacks and models for demonstration.
  - `scripts/calibrate_defense.py`: To access detection defenses for threshold calibration.

**Implementation Notes**
- **Architecture decisions**: Uses a facade pattern to hide the internal file structure of the `core` package (`attacks.py`, `defenses.py`, `llm_wrapper.py`) from external users, allowing imports like `from src.core import CombinedAttack`.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/data/__init__.py; ROUND 16 
================================================================================

# Code Implementation Summary
**Generated**: 2025-12-08 10:32:15
**File Implemented**: src/data/__init__.py

**Core Purpose**
This file serves as the package initializer for the `src.data` module, exposing the primary data loading and sampling classes to simplify imports throughout the rest of the application.

**Public Interface**
- **Classes Exported**:
  - `DataLoader`: Handles loading and formatting of datasets for the 7 supported tasks.
  - `TaskSampler`: Manages the creation of non-overlapping target/injected pairs for experiments.

**Internal Dependencies**
- From `.loader`: Imports `DataLoader`
- From `.sampler`: Imports `TaskSampler`

**External Dependencies**
- **Imported by**:
  - `scripts/run_benchmark.py`: To load data and sample pairs for the main experiment loop.
  - `scripts/calibrate_defense.py`: To load clean data for PPL threshold calibration.
  - `scripts/demo_attack.py`: To load a single example for demonstration.

**Implementation Notes**
- **Architecture**: Uses the `__all__` list to explicitly define the public API of the data package, keeping internal helper functions (if any were added later) private by default.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/evaluation/__init__.py; ROUND 17 
================================================================================

# Code Implementation Summary
**Generated**: 2025-12-08 10:32:33
**File Implemented**: src/evaluation/__init__.py

**Core Purpose**
This file initializes the `src.evaluation` package, exposing the primary classes for metric calculation and defense threshold calibration to simplify imports for benchmark scripts.

**Public Interface**
- **Exports**:
  - Class `MetricsCalculator`: Handles calculation of ASV, MR, PNA, and other NLP metrics.
  - Class `ThresholdCalibrator`: Manages the calibration of perplexity thresholds for detection defenses.

**Internal Dependencies**
- From `.metrics`: `MetricsCalculator`
- From `.calibrator`: `ThresholdCalibrator`

**External Dependencies**
- Expected to be imported by: `scripts/run_benchmark.py`, `scripts/calibrate_defense.py`
- Key exports used elsewhere: `MetricsCalculator`, `ThresholdCalibrator`

**Implementation Notes**
- Architecture decisions: Uses `__all__` to explicitly define the public API of the evaluation package, hiding internal implementation details if necessary in the future.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/utils/__init__.py; ROUND 18 
================================================================================

# Code Implementation Summary
**Generated**: 2025-12-08 10:32:51
**File Implemented**: src/utils/__init__.py

**Core Purpose**
This file initializes the `src.utils` package, exposing essential text processing and logging utility functions to the rest of the application to simplify import statements and define the public API of the utilities module.

**Public Interface**
- Function `clean_text`: Imported from `.text_processing` (cleans and normalizes text).
- Function `format_prompt`: Imported from `.text_processing` (formats prompts for LLMs).
- Function `setup_logger`: Imported from `.logger` (configures application-wide logging).

**Internal Dependencies**
- From `.text_processing`: `clean_text`, `format_prompt`
- From `.logger`: `setup_logger`

**External Dependencies**
- Expected to be imported by: `src/core/llm_wrapper.py`, `src/data/loader.py`, `scripts/run_benchmark.py`, and other core components requiring standardized logging or text manipulation.

**Implementation Notes**
- Architecture decisions: Uses `__all__` to explicitly define the exported namespace, preventing internal implementation details from being exposed when using `from src.utils import *`.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File README.md; ROUND 19 
================================================================================

# Code Implementation Summary
**Generated**: 2025-12-08 10:33:30
**File Implemented**: README.md

**Core Purpose**
Provides comprehensive documentation for the "Formalizing and Benchmarking Prompt Injection Attacks and Defenses" reproduction project, serving as the primary guide for installation, usage, and understanding the project architecture. It details how to reproduce the paper's experiments using the implemented scripts.

**Public Interface**
*   **CLI Usage Guide**:
    *   `scripts/calibrate_defense.py`: Instructions for calibrating PPL detection thresholds (Args: `--model_type`, `--fpr`, `--limit`).
    *   `scripts/run_benchmark.py`: Instructions for running the 7x7 evaluation matrix (Args: `--model_type`, `--num_samples`, `--attacks`, `--defenses`).
    *   `scripts/demo_attack.py`: Instructions for visualizing single attack instances (Args: `--target_task`, `--injected_task`, `--attack`).
*   **Configuration Guide**:
    *   Directs users to `config/models.yaml` for model setup (OpenAI/Local).
    *   Directs users to `config/tasks.yaml` for task definitions.

**Internal Dependencies**
*   References `requirements.txt` for dependency installation.
*   References the `scripts/` directory for execution entry points.
*   References the `config/` directory for system configuration.

**External Dependencies**
*   None (This file is the root documentation consumed by the end-user).

**Implementation Notes**
*   **Scope**: Explicitly lists the 5 implemented attacks (Naive, Escape, Context Ignoring, Fake Completion, Combined) and 10 defenses (Prevention & Detection).
*   **Metrics**: Defines ASV (Attack Success Value) and MR (Matching Rate) to help users interpret results.
*   **Environment**: Notes specific requirements for local models (GPU VRAM) vs API models (OpenAI Key).

---
*Auto-generated by Memory Agent*


