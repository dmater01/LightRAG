# Formalizing and Benchmarking Prompt Injection Attacks and Defenses

This repository contains a complete reproduction of the framework described in the paper **"Formalizing and Benchmarking Prompt Injection Attacks and Defenses"**. It implements a formal mathematical framework for prompt injection, the novel 'Combined Attack' strategy, and a comprehensive benchmark of 5 attacks and 10 defenses across 7 NLP tasks.

## ğŸŒŸ Core Features

*   **5 Attack Strategies**: Naive, Escape Characters, Context Ignoring, Fake Completion, and the novel **Combined Attack**.
*   **10 Defense Strategies**:
    *   *Prevention*: Paraphrasing, Retokenization, Delimiters, Sandwich Prevention, Instructional Prevention.
    *   *Detection*: Perplexity (PPL) Detection, Windowed PPL Detection, Known-Answer (Canary) Detection.
*   **7 NLP Tasks**: MRPC, Jfleg, HSOL, RTE, SST2, SMS Spam, Gigaword.
*   **Unified LLM Interface**: Supports both OpenAI API (GPT-3.5/4) and local HuggingFace models (e.g., Llama-2).

## ğŸ› ï¸ Installation

1.  **Clone the repository** (if applicable) or ensure you are in the project root.

2.  **Install Dependencies**:
    ```bash
    pip install -r requirements.txt
    ```
    *Note: A GPU with at least 24GB VRAM is recommended for running Llama-2-13b locally. For smaller tests, you can configure smaller models in `config/models.yaml`.*

3.  **Environment Setup**:
    If you plan to use OpenAI models, set your API key:
    ```bash
    export OPENAI_API_KEY="your-api-key-here"
    ```

## ğŸš€ Usage

### 1. Calibrate Defenses (Recommended First Step)
Before running detection defenses (PPL-based), you must calibrate the thresholds on clean data to ensure a low False Positive Rate (FPR).

```bash
python scripts/calibrate_defense.py \
    --model_type local \
    --fpr 0.01 \
    --limit 100
```
This will generate `config/defense_thresholds.json`.

### 2. Run the Benchmark
Run the full 7x7 Task x Injection matrix evaluation.

```bash
python scripts/run_benchmark.py \
    --model_type local \
    --num_samples 100 \
    --output_dir results
```

**Options**:
*   `--attacks`: Comma-separated list (e.g., `combined,naive`). Default: all.
*   `--defenses`: Comma-separated list (e.g., `paraphrasing,ppl_detection`). Default: none.
*   `--target_tasks`: Specific target tasks (e.g., `sst2`).
*   `--injected_tasks`: Specific injected tasks (e.g., `sms`).

### 3. Demo Single Attack
Visualize how an attack works on a single example.

```bash
python scripts/demo_attack.py \
    --target_task sst2 \
    --injected_task sms \
    --attack combined \
    --model_type local
```

## ğŸ“‚ Project Structure

```
project_root/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ tasks.yaml            # Task definitions and prompts
â”‚   â”œâ”€â”€ models.yaml           # Model configurations
â”‚   â””â”€â”€ defense_thresholds.json # Generated by calibration script
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ attacks.py        # Attack implementations
â”‚   â”‚   â”œâ”€â”€ defenses.py       # Defense implementations
â”‚   â”‚   â””â”€â”€ llm_wrapper.py    # OpenAI/Local model wrapper
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ loader.py         # Dataset loading
â”‚   â”‚   â””â”€â”€ sampler.py        # Pair sampling logic
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ metrics.py        # ASV, MR, Rouge, Accuracy
â”‚   â”‚   â””â”€â”€ calibrator.py     # PPL threshold calibration
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ text_processing.py
â”‚       â””â”€â”€ logger.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_benchmark.py      # Main experiment runner
â”‚   â”œâ”€â”€ calibrate_defense.py  # Calibration script
â”‚   â””â”€â”€ demo_attack.py        # Demo script
â””â”€â”€ requirements.txt
```

## ğŸ“Š Metrics

The benchmark reports two key metrics:
1.  **ASV (Attack Success Value)**: The rate at which the model follows the *injected* instruction instead of the target instruction.
2.  **MR (Matching Rate)**: The similarity between the attacked output and the output of the model if asked the injected task directly.

## ğŸ“ Configuration

*   **Models**: Edit `config/models.yaml` to change model paths (e.g., to use `meta-llama/Llama-2-7b-chat-hf`) or generation parameters.
*   **Tasks**: Edit `config/tasks.yaml` to modify prompts or dataset mappings.

## âš ï¸ Note on Local Models
The default configuration assumes access to HuggingFace models. You may need to log in via `huggingface-cli login` to access gated models like Llama-2.
